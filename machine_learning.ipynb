{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python for Neuroimaging\n",
    "\n",
    "This notebook, prepared by Désirée Lussier, is to provide an introduction to machine learning for BrainHack School 2022. It has been adapted from Brainhack Global MTL 2019 traintrack tutorial (https://github.com/BrainhackMTL/global2019-traintrack/blob/master/machine_learning.ipynb) by Désirée Lussier which was modified from the MAIN 2019 training given by Alexadre Hutton (https://github.com/main-training/main-training-nilearn-ml/blob/master/01_intro_ml.ipynb; https://github.com/main-training/main-training-nilearn-ml/blob/master/01_intro_ml_slides.odp) and sklearn tutorial (http://scipy-lectures.org/packages/scikit-learn/index.html).\n",
    "\n",
    "This training is meant to give a brief overview of the basics of machine learning in Python3. For more complete training or other specific examples please see the additional BrainHack School modules (https://school.brainhackmtl.org/modules/) above MAIN courses (https://github.com/main-training) and documentation for Scikit Learn (https://scikit-learn.org/stable/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Outline\n",
    "\n",
    "## Machine Learning with Scikit Learn\n",
    "\n",
    "* Machine learning classification examples\n",
    "* Model evaluation\n",
    "* Model complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning?\n",
    "In machine learning models with parameters which are optimized according to previously-seen data.\n",
    "\n",
    "Machine learning (ML) can be divided into subcategories:\n",
    "\n",
    "## Supervised Learning\n",
    "We have observations X we want to use to predict Y\n",
    "\n",
    "X: data, features, inputs\n",
    "Y: target, labels, outputs\n",
    "The goal is to find a model which best predicts Y based on X:\n",
    "Y = f(X)\n",
    "\n",
    "Models are further divided:\n",
    "* Classification: Predicting ordinal numbers; determining classes for inputs\n",
    "* Regression: Predicting continuous values\n",
    "\n",
    "## Unsupervised Learning\n",
    "We have observations X\n",
    "The goal is to extract information about X\n",
    "E.g.: finding a representation, cluster the data\n",
    "\n",
    "ML models are typically developed in some variation of:\n",
    "* Parameter training\n",
    "* Model evaluation\n",
    "* Model selection\n",
    "* Model generalization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "## Typical ML Pipeline\n",
    "\n",
    "### Model fitting\n",
    "\n",
    "Data exploration -> rnd. data split (whole dataset) -> training set -> parameters optimization (on training data only) -> trained model \n",
    "-> test set -> evaluate on test set\n",
    "\n",
    "1. Model (estimator) selection, e.g., linear regression, polynomial regression, multi-layer perception (ANN), etc.\n",
    "2. Loss (cost) function, e.g., mean squared error, mean absolute error, max error, explained variation, R-squared, etc.\n",
    "3. Optimization of model parameter! \n",
    "y= ax+b, Loss = f(a,b) to find the value to minimize the loss value\n",
    "\n",
    "Gradient Descent: \n",
    "* Initialize model parameters (a,b) randomly\n",
    "* Iterate between:\n",
    "    * Compute loss, e.g., MSE\n",
    "    * Update model parameters (a,b) in direction of gradient with training data\n",
    "\n",
    "### Model Validation\n",
    "\n",
    "1. Use Test data to verify the model\n",
    "2. Score function, e.g., mean squared error, mean absolute error, max error, explained variation, R-squared, etc.\n",
    "\n",
    "### k-fold cross-validation\n",
    "* To Avoid overfitting\n",
    "* Improve model generalizability\n",
    "* Provide a more accurate performance measure\n",
    "* Hyper-paramter tuning\n",
    "\n",
    "1. Train model parameters on training set\n",
    "2. Evaluate training with the validation set, which is the subset of training set\n",
    "3. Report error on test set only at the very end\n",
    "\n",
    "### How many Training Epochs? \n",
    "* When the loss function starts increasing on the validation set, that's where you should stop training\n",
    "* Underfitting: A statistical model or a machine learning algorithm cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data -> Low variance but away from the target\n",
    "* Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data -> Close to the target but high variance\n",
    "* Bias-Variance trade-off: error(val/test) = bias + variance\n",
    "\n",
    "## How to improve models\n",
    "\n",
    "### Dimensionality Reduction\n",
    "* After split train set before the parameters optimization\n",
    "* Curse of dimentionality (more features than samples)\n",
    "* Intrinsic dimension may actually be small (redundante data)\n",
    "* Extract \"salient\" features\n",
    "* Remove noisy features\n",
    "\n",
    "1. Feature extraction/engineering\n",
    "    * Compact representation of the data\n",
    "    * Maps input features into a lower dimensional space\n",
    "    * Linear: PCA, ICA, LDA\n",
    "    * Nonlinear: Isomap, SOM, autoencoders, t-SNE\n",
    "2. Feature selection\n",
    "    * selection of a subet of input features\n",
    "    * Features are still in original space\n",
    "    * Variable selection: t-statistic\n",
    "    * Subset selection: min redundancy, max relevancy (mRMR)\n",
    "    * Other criteria: BIC, consistency, etc\n",
    "\n",
    "### Regularization\n",
    "* Penalties on the LOSS function to prevent overfitting!\n",
    "    1. L1/Lasso: constrains parameters to be sparse\n",
    "    2. L2/Ridge: constrains parameters to be small\n",
    "\n",
    "### Ensemble Methods\n",
    "* Bagging: bootstrpp resampling + aggregation\n",
    "* Boosting: learners learn sequentially and adaptively to improve model predictions of a learning algorithm\n",
    "\n",
    "## Classification\n",
    "* Support Vector Machine (SVM) \n",
    "--------------------------\n",
    "Probabilistic classifiers\n",
    "* Artificial Neural Metworks\n",
    "* Logistic regression\n",
    "* Decision Trees\n",
    "* Random Forests \n",
    "\n",
    "### Performance Metrics - Binary Classification\n",
    "|Prediction\\True    |Positive       | Negative      |\n",
    "|---                |---            |---            |\n",
    "|Positive           |True positive  |False positive |\n",
    "|Negative           |False negative |True negative  |\n",
    "\n",
    "* Score function:\n",
    "    1. Accuracy: Tp+Tn/(Tp+Tn+Fp+Fn)\n",
    "    2. Precision: Tp/(Tp+Fp)\n",
    "    3. Recall: Tp/(Tp+Fp)\n",
    "    4. F1 score: 2Tp/(2Tp+Fp+Fn)\n",
    "* Precision and Recall are appropriate when classes are imbalanced!\n",
    "----\n",
    "Probabilistic classifiers\n",
    "* Score function:\n",
    "    1. ROC curve\n",
    "    2. precision-recall curve -> Appropriate when classes are imbalanced!\n",
    "\n",
    "### Multiclass Prediction\n",
    "* To extend a binary metric to multiclass problems, the data is treated as a collection of binary problems, one for each class.\n",
    "* The binary metric is then averaged across the set of classes, each of which may be useful in some scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
