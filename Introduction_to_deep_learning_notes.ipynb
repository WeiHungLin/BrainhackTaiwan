{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal of This Talk\n",
    "- To give a sense of what deep learning is actually good for, and what it's not good for\n",
    "- To give a sense of when deep learning is the right strategy to take in your research\n",
    "\n",
    "## Outline\n",
    "1. No free lunch in AI\n",
    "2. Inductive biases and the AI set\n",
    "3. Deep architectures as a good inductive bias\n",
    "4. Making learning work in deep architectures\n",
    "5. When to use deep learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Free Lunch in AI\n",
    "The early dream of machine learning researchers was that we could develop general purpose learning algorithms that could learn well in any scenario on any dataset.\n",
    "\n",
    "However, there exists no learning algorithm that can perform better than all other learning algorithms on all tasks. \n",
    "\n",
    "**No Free Lunch Theorem for Optimization (Wolpert and MacReady, 1997)**\n",
    "- You can find the most appropriate algorithm for different problems, and each algorithm has its own applicable problems.\n",
    "- Make sure you completely understand a machine learning problem and the data involved before selecting an algorithm to use.\n",
    "- All models are only as good as the assumptions that they were created with and the data that was used to train them.\n",
    "- Simpler models like logistic regression have more bias and tend to underfit, while more complex models like neural networks have more variance and tend to overfit.\n",
    "- The best models for a given problem are somewhere in the middle of the two bias-variance extremes.\n",
    "- To find a good model for a problem, you may have to try different models and compare them using a robust cross-validation strategy. \n",
    "\n",
    "If you don't any expectations about what task you're going to apply your algorithms, you are never able to actually have an algorithm that outperforms all other algorithms on average.\n",
    "\n",
    "**You have to assume a non-flat prior over loss functions and select an algorithm that is well suited to learning that specific task if you want best performance**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inductive Biases And the AI Set\n",
    "How to select the right algorithms for AI?\n",
    "\n",
    "Three typical response categories\n",
    "\n",
    "1. Defeatism: We just have to hand-craft specific solutions to very specific task we ever want to accomplish in AI.\n",
    "2. Denial: Things like kernel machines can approximate any function, and we have bounds on their ability to generalize with regularization, so it's all good.\n",
    "3. Optimism: We can define the set of things we actually want to do with AI and design systems that are general purpose within that restricted set. But, this requires inductive biases.\n",
    "\n",
    "### Inductive Biases\n",
    "Inductive biases are assumptions that we bake into our algorithms about the sort of tasks we will be performing. They are a means of embedding prior knoweldge into an optimization system.\n",
    "\n",
    "Broadly, we can build inductive biases into machine learning algorithms using the following three components:\n",
    "1. Using hand-wired pre-processing of data (e.g., extracting pre-determined features)\n",
    "2. Using specific architectures for our learning machines\n",
    "3. Using specific loss functions and regularizers for our learning machines\n",
    "\n",
    "Note: a regurlarizer is just a way of preventing over-fitting to data.\n",
    "\n",
    "### The AI Set\n",
    "AI should take inspiration from brains!\n",
    "\n",
    "Our brains seem general purpose in their learning, but they are actually pretty restricted, and learn certain things far more easily than others (e.g., learning to read natural language is easier than learning to read barcodes).\n",
    "\n",
    "AI should be concerned with the broad set of tasks that animals and people are good at, and maybe some related tasks that animals and people aren't good at only to physical/speed limitations.\n",
    "\n",
    "AI research should be about defining good inductive biases that are as minimal as possible in order to do well on the AI set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Architectures as A Good Inductive Biases\n",
    "### Kernel machines\n",
    "Kernel methods are types of algorithms that are used for pattern analysis. These methods involve using linear classifiers to solve nonlinear problems.\n",
    "\n",
    "**Getting good features**\n",
    "The basic idea: project the data into a high-dimensional space where it becomes linearly separable.\n",
    "\n",
    "The most obvious way to do this is to try to define a set of pre-processing stages that will accomplish this.\n",
    "\n",
    "Kernal machines try to avoid hand-crafting the pre-processing stages by instead using the data itself to construct the pre-processing. But it turns out that kernel machines do not actually perform that well.\n",
    "\n",
    "### The world is hierarchical\n",
    "The world around us is compositional. This means that when we experience something, it is usally composed of smaller pieces, that are themselves composed of smaller pieces, etc. \n",
    "\n",
    "### AI should be hierarchical\n",
    "If the world is best understood in terms of compositions of different pieces, then what we want is not a single non-linear projection into a new space (per kernel machines).\n",
    "\n",
    "Instead, we want multiple layers of non-linear functions, each one operating with the features identified by the lower layers.\n",
    "\n",
    "This is an inductive bias. It says, essentially, that we assme tasks where we have to build hierarchies of features.\n",
    "\n",
    "We need to devise AI architectures that have built-in hierarchy. In other words, we need deep architectures. This line of logic is the origins of \"deep learning\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Learning Work in Deep Architectures\n",
    "### Towards deep learning: gradient descent\n",
    "You want to learn a hierarchy, and the most successful approach has been gradient descent to find the global cost minimum.\n",
    "\n",
    "In the right circumstances, gradient descent in deep networks can work really well.\n",
    "\n",
    "### Size matters\n",
    "**The strange double descent phenomenon**\n",
    "In statistics and machine learning, double descent is the phenomenon where a statistical model with a small number of parameters and a model with an extremely large number of parameters have a small error, but a model whose number of parameters is about the same as the number of data points used to train the model will have a large error.\n",
    "\n",
    "### Other things that help with deep learning\n",
    "- Modifications to gradient descent to escape saddles (e.g., ADAM)\n",
    "- Activation functions that mitigate vanishing gradients (e.g., ReLU)\n",
    "- Good regularization schemes (e.g., Dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Deep Learning\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
