{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MitSrB2gW4x4"
      },
      "source": [
        "# [nobrainer](https://github.com/neuronets/nobrainer)\n",
        "\n",
        "A framework for developing neural network models for 3D image processing.\n",
        "\n",
        "Nobrainer includes guide notebooks on its GitHub page. This notebook is different in that it combines several concepts into a single notebook.\n",
        "\n",
        "If you come across any issues or ways to improve nobrainer, please submit an [issue](https://github.com/neuronets/nobrainer/issues) or [pull request](https://github.com/neuronets/nobrainer/pulls).\n",
        "\n",
        "See pre-trained models at https://github.com/neuronets/nobrainer-models, https://github.com/neuronets/kwyk, and https://github.com/neuronets/ams.\n",
        "\n",
        "# This notebook\n",
        "\n",
        "**Lots of breadth, some depth.**\n",
        "\n",
        "We will...\n",
        "1. Learn how to visualize MRI data prior to model training\n",
        "2. Learn how to process features for model training\n",
        "3. Train a brain extraction model\n",
        "4. Use a pre-trained nobrainer model for brain extraction and another for multi-class segmentation.\n",
        "5. Understand the parts of a real-world deep learning workflow\n",
        "6. Use nobrainer to construct that workflow and train a brain-extraction model using transfer learning.\n",
        "\n",
        "The vast majority of this can be done on your personal laptop with a CPU (no GPU required). There are two parts in which you would benefit from a GPU (for model training), but the outputs are pasted in those sections in case you do not have a GPU.\n",
        "\n",
        "# Wisdom\n",
        "\n",
        "- Garbage in, garbage out\n",
        "- Deep learning is not a hammer -- it is not always the right tool for the job\n",
        "- Be patient -- you _will_ run into errors and difficulties\n",
        "- Be collaborative -- if you find ways to improve Nobrainer, submit an [issue](https://github.com/neuronets/nobrainer/issues) or [pull request](https://github.com/neuronets/nobrainer/pulls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbKl7KnMW4x6"
      },
      "source": [
        "## Google Colaboratory\n",
        "\n",
        "Notebooks can be for free on Google Colaboratory (you must be signed into a Google account). If you are using Colab, please note that multiple open tabs of Colab notebooks will use the same resources (RAM, GPU). Downloading data in multiple Colab notebooks at the same time or training multiple models can quickly exhaust the available resources. For this reason, please run one notebook at a time, and keep an eye on the resources used.\n",
        "\n",
        "Users can choose to run Colab notebooks on CPU, GPU, or TPU. By default, the notebooks will use the CPU runtime. To use a different runtime, please select `Runtime > Change runtime type` in the menu bar. Then, choose either `GPU` or `TPU` under `Hardware accelerator`. No code changes are required when running on CPU or GPU runtime. When using the TPU runtime, however, special care must be taken for things to work properly. Please refer to the TPU guide notebook in this directory for more information.\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        "These notebooks can use whatever hardware you have available, whether it is CPU, GPU, or TPU. Please note that training models on CPU can take a very long time. GPUs will greatly increase speed of training and inference. Some of the notebooks download example data, but you can feel free to use your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc7fMcNBW4x7"
      },
      "source": [
        "# Install requirements\n",
        "\n",
        "You will need nobrainer and tensorflow. We will install nobrainer from a particular git commit.\n",
        "\n",
        "Use the following command to install:\n",
        "\n",
        "```\n",
        "python -m pip install --no-cache-dir \\\n",
        "    tensorflow \\\n",
        "    https://github.com/neuronets/nobrainer/tarball/0033386e9a603a6987b9667fe9942c0988f096c4\n",
        "```\n",
        "\n",
        "If you are using jupyter notebook on your local machine, you might consider creating a new conda environment for this lesson so that you can tear it down (if you really want to) after it's over.\n",
        "\n",
        "```\n",
        "conda create --name tf python=3\n",
        "conda activate tf\n",
        "python -m pip install --no-cache-dir \\\n",
        "    tensorflow \\\n",
        "    https://github.com/neuronets/nobrainer/tarball/0033386e9a603a6987b9667fe9942c0988f096c4\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRQ7nEA3YLkQ",
        "outputId": "7f9fec2d-b0a4-4f59-feb3-2cd07a08e930"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --no-cache-dir \\\n",
        "    tensorflow \\\n",
        "    https://github.com/neuronets/nobrainer/tarball/0033386e9a603a6987b9667fe9942c0988f096c4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVk-DLUlW4x7"
      },
      "outputs": [],
      "source": [
        "import nobrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDWGrz57W4x8"
      },
      "source": [
        "### Layout\n",
        "\n",
        "- `nobrainer.io`: input/output methods\n",
        "- `nobrainer.layers`: custom Keras layers\n",
        "- `nobrainer.losses`: loss functions for volumetric segmentation\n",
        "- `nobrainer.metrics`: metrics for volumetric segmentation\n",
        "- `nobrainer.models`: pre-defined Keras models\n",
        "- `nobrainer.tfrecords`: writing and reading of TFRecords files\n",
        "- `nobrainer.transform`: rigid transformations for data augmentation\n",
        "- `nobrainer.volume`: `tf.data.Dataset` creation and data augmentation utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PiEGEY1W4x8"
      },
      "source": [
        "# Let's dive in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXaz_GLDW4x8"
      },
      "source": [
        "# Get data\n",
        "\n",
        "T1-weighted MRI and corresponding multi-class segmentation.\n",
        "\n",
        "Made using FreeSurfer `recon-all`. High quality segmentation, but takes over four hours per brain.\n",
        "\n",
        "Come from https://datasets.datalad.org/workshops/nih-2017/ds000114"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB8c8wUnW4x8",
        "outputId": "dfd1263a-51cd-41bd-de09-477ce8b98fc5"
      },
      "outputs": [],
      "source": [
        "csv_path = nobrainer.utils.get_data()\n",
        "\n",
        "filepaths = nobrainer.io.read_csv(csv_path)\n",
        "print((\"features\", \"labels\"))\n",
        "for row in filepaths:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1aZH1iTW4x9"
      },
      "source": [
        "# Understand your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX6fnrSdW4x9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Print floats with fixed-point notation (not scientific notation)\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_08YDe4W4x9"
      },
      "outputs": [],
      "source": [
        "pair_of_filepaths = filepaths[0]\n",
        "x = nobrainer.io.read_volume(pair_of_filepaths[0])\n",
        "y = nobrainer.io.read_volume(pair_of_filepaths[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DATh4mYWW4x9",
        "outputId": "85735be6-00a7-4276-ac48-4fbc87c7f822"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBlrInqbW4x9",
        "outputId": "bea588f4-ba69-4052-c4a7-f8027eb9a71e"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_F6petW4x9",
        "outputId": "b760fe34-895e-4029-b456-5da85fa447b8"
      },
      "outputs": [],
      "source": [
        "x.min(), x.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exT3BPNvW4x9",
        "outputId": "70c09427-8f5a-42c5-9363-d107b82fb4c7"
      },
      "outputs": [],
      "source": [
        "y.min(), y.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwO6wamkW4x9",
        "outputId": "6c2d9297-59b1-45d5-b67e-896b657ff5f3"
      },
      "outputs": [],
      "source": [
        "np.unique(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivj8ya18W4x-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "TAlKpBoUW4x-",
        "outputId": "c85220fd-bccc-493d-b66a-a1bb651320ba"
      },
      "outputs": [],
      "source": [
        "plt.hist(x.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "w5qYwUctW4x-",
        "outputId": "fee783e7-772d-4fc5-d54a-fb96096e6f82"
      },
      "outputs": [],
      "source": [
        "plt.hist(x.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Hq6c-qmJW4x-",
        "outputId": "9f4daa44-3f24-464a-dd3b-728598e2da64"
      },
      "outputs": [],
      "source": [
        "plt.hist(y.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj2wECHdW4x-",
        "outputId": "6bb247f5-f7aa-4105-f98a-004d1a9da91c"
      },
      "outputs": [],
      "source": [
        "print(\"value,count\")\n",
        "vals, counts = np.unique(y, return_counts=True)\n",
        "print(np.stack((vals, counts), axis=1))\n",
        "del vals, counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22gPWwHOW4x-"
      },
      "source": [
        "### Plot scan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "VPyc9J1MW4x-",
        "outputId": "70369aba-9b7d-461a-d775-65a9b9a0cf3f"
      },
      "outputs": [],
      "source": [
        "plt.matshow(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy8y1QK7W4x-",
        "outputId": "ee78b164-22bf-4d56-a285-6c6be43257e6"
      },
      "outputs": [],
      "source": [
        "x[120].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Z4feGPSjW4x-",
        "outputId": "212d9aed-e3b3-442b-f67b-0e16a52f84a5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
        "\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "\n",
        "axes[0].matshow(x[120], cmap=\"gray\")\n",
        "axes[0].set_title(\"Sagittal plane\")\n",
        "\n",
        "axes[1].matshow(x[:, 120], cmap=\"gray\")\n",
        "axes[1].set_title(\"Axial (transverse) plane\")\n",
        "\n",
        "axes[2].matshow(x[:, :, 120], cmap=\"gray\")\n",
        "axes[2].set_title(\"Coronal plane\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "A2jeAADgW4x_",
        "outputId": "4bb1941c-4b78-4d89-8d4c-c3eaf474b0ed"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
        "\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "\n",
        "axes[0].matshow(y[120], cmap=\"hot\")\n",
        "axes[0].set_title(\"Sagittal plane\")\n",
        "\n",
        "axes[1].matshow(y[:, 120], cmap=\"hot\")\n",
        "axes[1].set_title(\"Axial (transverse) plane\")\n",
        "\n",
        "axes[2].matshow(y[:, :, 120], cmap=\"hot\")\n",
        "axes[2].set_title(\"Coronal plane\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VVQmwgW4x_"
      },
      "source": [
        "## Make brainmask\n",
        "\n",
        "All of these labels comprise the brain, so we can collapse all of these values into one to make a brainmask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyTPZIb8W4x_"
      },
      "outputs": [],
      "source": [
        "# Binarize\n",
        "# Translation: in y, wherever value is greater than 0, set that value to 1.\n",
        "y[y > 0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "futCWRqTW4x_"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
        "\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "\n",
        "axes[0].matshow(y[120], cmap=\"hot\")\n",
        "axes[0].set_title(\"Sagittal plane\")\n",
        "\n",
        "axes[1].matshow(y[:, 120], cmap=\"hot\")\n",
        "axes[1].set_title(\"Axial (transverse) plane\")\n",
        "\n",
        "axes[2].matshow(y[:, :, 120], cmap=\"hot\")\n",
        "axes[2].set_title(\"Coronal plane\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dujTegEGW4x_"
      },
      "source": [
        "### Check overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoWVUB6GW4x_"
      },
      "outputs": [],
      "source": [
        "plt.matshow(x[:, 120], cmap=\"gray\", fignum=0)\n",
        "plt.axis(False)\n",
        "plt.matshow(y[:, 120], cmap=\"Purples\", fignum=0, alpha=0.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_Z2nJrW4x_"
      },
      "source": [
        "## Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXx7fbbNW4x_"
      },
      "source": [
        "## Process data\n",
        "\n",
        "- Standardize features\n",
        "    - many options for this... whatever you choose, be consistent\n",
        "- Separate data into smaller cubes, because our data are too big for all but the most expensive GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEGKw-nW4x_"
      },
      "outputs": [],
      "source": [
        "x = nobrainer.volume.standardize_numpy(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILv5iO1jW4x_",
        "outputId": "7ef96dc6-b1d6-451a-b20a-fa5e0aefebe2"
      },
      "outputs": [],
      "source": [
        "x.mean(), x.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5U23uLIW4x_"
      },
      "outputs": [],
      "source": [
        "block_shape = (128, 128, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srvFm3y1W4x_"
      },
      "outputs": [],
      "source": [
        "x_blocks = nobrainer.volume.to_blocks_numpy(x, block_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHLG69SqW4x_"
      },
      "outputs": [],
      "source": [
        "y_blocks = nobrainer.volume.to_blocks_numpy(y, block_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcLH4cGoW4x_",
        "outputId": "2517de29-fe10-4cfa-84e4-94700cc7e250"
      },
      "outputs": [],
      "source": [
        "x_blocks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxB7NLqBW4x_",
        "outputId": "ddc3dea6-af38-4bb9-e73c-50e1b2a17b77"
      },
      "outputs": [],
      "source": [
        "y_blocks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86XtLWhW4yA",
        "outputId": "13623dd3-4696-4f8a-fc47-070ac87c926e"
      },
      "outputs": [],
      "source": [
        "# Add grayscale channel\n",
        "x_blocks = np.expand_dims(x_blocks,  axis=-1)\n",
        "x_blocks.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-jQNqNnW4yA",
        "outputId": "7c6a0df6-ca55-4b02-e298-7b67f4f29e44"
      },
      "outputs": [],
      "source": [
        "# Add dimension to labels (to match the output shape of the model)\n",
        "y_blocks = np.expand_dims(y_blocks, axis=-1)\n",
        "y_blocks.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8wWmLBdW4yA"
      },
      "source": [
        "## Instantiate model\n",
        "\n",
        "Lots of freedom here. I am not in the market of creating new models. I refer to the literature to see what works.\n",
        "\n",
        "Input shape is block shape and channel. The batch dimension (the first dimension) is variable, and we don't provide it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU9oIAkGW4yA",
        "outputId": "87731214-63f3-4b08-d025-502766e9558e"
      },
      "outputs": [],
      "source": [
        "input_shape = (*block_shape, 1)\n",
        "input_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLbEVvrdW4yA"
      },
      "source": [
        "### U-Net architecture\n",
        "\n",
        "It looks like a U ....\n",
        "\n",
        "See the Nobrainer implementation at https://github.com/neuronets/nobrainer/blob/master/nobrainer/models/unet.py\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm7IsIowW4yA"
      },
      "outputs": [],
      "source": [
        "model = nobrainer.models.unet(\n",
        "    n_classes=1, \n",
        "    input_shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5489v_w0W4yA",
        "outputId": "f36866a0-f030-4d53-df0d-52dad6ae5954",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvfJQoSyW4yA"
      },
      "source": [
        "### Configure the model\n",
        "\n",
        "This is where you choose the optimizer, the loss function, metrics, and other optional things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvpDvBZ4W4yA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of6R0iI6W4yA"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-04),\n",
        "    loss=nobrainer.losses.dice,\n",
        "    metrics=[nobrainer.metrics.dice]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16TebquhW4yA"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "This will be _very_ slow if you do not have a GPU.\n",
        "\n",
        "#### Training is a cost-benefit between different forces\n",
        "- learning rate: a lower learning rate will result in slower training, but a high learning rate might destroy the learned weights.\n",
        "- batch size: a larger batch size is typically better because your model sees more data at once, but larger batch sizes require hardware with more memory. In 3D segmentation, we typically need to use low batch sizes because of memory limitations.\n",
        "- block shape: larger blocks are typically better, because your model sees more of the data in each block, but larger blocks require more memory.\n",
        "\n",
        "#### Definitions\n",
        "- step: one pass forward (and backward) through the model\n",
        "- batch_size: number of samples your model sees in one step\n",
        "- epoch: one pass through your training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm2vEmPnW4yA"
      },
      "source": [
        "Here are the logs from training if you cannot train on your own. Notice that the Dice score increases slowly. The loss is `1 - dice`. A key takeaway from this is that training is iterative and slow.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "<details>\n",
        "    Train on 8 samples\n",
        "    Epoch 1/50\n",
        "    8/8 [==============================] - 18s 2s/sample - loss: 0.8979 - dice: 0.1021\n",
        "    Epoch 2/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.8899 - dice: 0.1101\n",
        "    Epoch 3/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.8808 - dice: 0.1192\n",
        "    Epoch 4/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.8673 - dice: 0.1327\n",
        "    Epoch 5/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.8487 - dice: 0.1513\n",
        "    Epoch 6/50\n",
        "    8/8 [==============================] - 6s 760ms/sample - loss: 0.8314 - dice: 0.1686\n",
        "    Epoch 7/50\n",
        "    8/8 [==============================] - 6s 762ms/sample - loss: 0.8121 - dice: 0.1879\n",
        "    Epoch 8/50\n",
        "    8/8 [==============================] - 6s 763ms/sample - loss: 0.7683 - dice: 0.2317\n",
        "    Epoch 9/50\n",
        "    8/8 [==============================] - 6s 762ms/sample - loss: 0.6592 - dice: 0.3408\n",
        "    Epoch 10/50\n",
        "    8/8 [==============================] - 6s 766ms/sample - loss: 0.5488 - dice: 0.4512\n",
        "    Epoch 11/50\n",
        "    8/8 [==============================] - 6s 763ms/sample - loss: 0.5081 - dice: 0.4919\n",
        "    Epoch 12/50\n",
        "    8/8 [==============================] - 6s 766ms/sample - loss: 0.4432 - dice: 0.5568\n",
        "    Epoch 13/50\n",
        "    8/8 [==============================] - 6s 763ms/sample - loss: 0.4276 - dice: 0.5724\n",
        "    Epoch 14/50\n",
        "    8/8 [==============================] - 6s 770ms/sample - loss: 0.5131 - dice: 0.4869\n",
        "    Epoch 15/50\n",
        "    8/8 [==============================] - 6s 764ms/sample - loss: 0.5177 - dice: 0.4823\n",
        "    Epoch 16/50\n",
        "    8/8 [==============================] - 6s 767ms/sample - loss: 0.4676 - dice: 0.5324\n",
        "    Epoch 17/50\n",
        "    8/8 [==============================] - 6s 762ms/sample - loss: 0.3437 - dice: 0.6563\n",
        "    Epoch 18/50\n",
        "    8/8 [==============================] - 6s 763ms/sample - loss: 0.2927 - dice: 0.7073\n",
        "    Epoch 19/50\n",
        "    8/8 [==============================] - 6s 762ms/sample - loss: 0.2700 - dice: 0.7300\n",
        "    Epoch 20/50\n",
        "    8/8 [==============================] - 6s 759ms/sample - loss: 0.2469 - dice: 0.7531\n",
        "    Epoch 21/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.2139 - dice: 0.7861\n",
        "    Epoch 22/50\n",
        "    8/8 [==============================] - 6s 755ms/sample - loss: 0.2087 - dice: 0.7913\n",
        "    Epoch 23/50\n",
        "    8/8 [==============================] - 6s 755ms/sample - loss: 0.2023 - dice: 0.7977\n",
        "    Epoch 24/50\n",
        "    8/8 [==============================] - 6s 755ms/sample - loss: 0.1975 - dice: 0.8025\n",
        "    Epoch 25/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1908 - dice: 0.8092\n",
        "    Epoch 26/50\n",
        "    8/8 [==============================] - 6s 761ms/sample - loss: 0.1864 - dice: 0.8136\n",
        "    Epoch 27/50\n",
        "    8/8 [==============================] - 6s 761ms/sample - loss: 0.1878 - dice: 0.8122\n",
        "    Epoch 28/50\n",
        "    8/8 [==============================] - 6s 759ms/sample - loss: 0.1842 - dice: 0.8158\n",
        "    Epoch 29/50\n",
        "    8/8 [==============================] - 6s 758ms/sample - loss: 0.1767 - dice: 0.8233\n",
        "    Epoch 30/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.1723 - dice: 0.8277\n",
        "    Epoch 31/50\n",
        "    8/8 [==============================] - 6s 755ms/sample - loss: 0.1657 - dice: 0.8343\n",
        "    Epoch 32/50\n",
        "    8/8 [==============================] - 6s 758ms/sample - loss: 0.1702 - dice: 0.8298\n",
        "    Epoch 33/50\n",
        "    8/8 [==============================] - 6s 761ms/sample - loss: 0.1604 - dice: 0.8396\n",
        "    Epoch 34/50\n",
        "    8/8 [==============================] - 6s 760ms/sample - loss: 0.1594 - dice: 0.8406\n",
        "    Epoch 35/50\n",
        "    8/8 [==============================] - 6s 759ms/sample - loss: 0.1625 - dice: 0.8375\n",
        "    Epoch 36/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.1506 - dice: 0.8494\n",
        "    Epoch 37/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1488 - dice: 0.8512\n",
        "    Epoch 38/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1558 - dice: 0.8442\n",
        "    Epoch 39/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1565 - dice: 0.8435\n",
        "    Epoch 40/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1683 - dice: 0.8317\n",
        "    Epoch 41/50\n",
        "    8/8 [==============================] - 6s 757ms/sample - loss: 0.1540 - dice: 0.8460\n",
        "    Epoch 42/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1405 - dice: 0.8595\n",
        "    Epoch 43/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1339 - dice: 0.8661\n",
        "    Epoch 44/50\n",
        "    8/8 [==============================] - 6s 760ms/sample - loss: 0.1288 - dice: 0.8712\n",
        "    Epoch 45/50\n",
        "    8/8 [==============================] - 6s 760ms/sample - loss: 0.1197 - dice: 0.8803\n",
        "    Epoch 46/50\n",
        "    8/8 [==============================] - 6s 758ms/sample - loss: 0.1284 - dice: 0.8716\n",
        "    Epoch 47/50\n",
        "    8/8 [==============================] - 6s 761ms/sample - loss: 0.1523 - dice: 0.8477\n",
        "    Epoch 48/50\n",
        "    8/8 [==============================] - 6s 756ms/sample - loss: 0.1531 - dice: 0.8469\n",
        "    Epoch 49/50\n",
        "    8/8 [==============================] - 6s 754ms/sample - loss: 0.1858 - dice: 0.8142\n",
        "    Epoch 50/50\n",
        "    8/8 [==============================] - 6s 753ms/sample - loss: 0.1658 - dice: 0.8342\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcs2IY2hW4yA",
        "outputId": "49f01f84-fffa-4b6f-e078-81ec33bb5b29"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x=x_blocks, \n",
        "    y=y_blocks, \n",
        "    batch_size=2, \n",
        "    epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "EgPHxeZhW4yA",
        "outputId": "516d57ac-231b-4f07-83f7-9f91d67501a8"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss value (1-Dice)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cPtHCquW4yB"
      },
      "source": [
        "### Try out the model on a new scan\n",
        "\n",
        "We _MUST_ process our new scan in the same way as we processed the training data.\n",
        "\n",
        "Here are the outputs if you were not able to train the model yourself\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyIdz8-BW4yB"
      },
      "outputs": [],
      "source": [
        "pair_of_filepaths = filepaths[1]\n",
        "xtest = nobrainer.io.read_volume(pair_of_filepaths[0])\n",
        "ytest = nobrainer.io.read_volume(pair_of_filepaths[1])\n",
        "\n",
        "xtest = nobrainer.volume.standardize_numpy(xtest)\n",
        "xtest_blocks = nobrainer.volume.to_blocks_numpy(xtest, block_shape)\n",
        "xtest_blocks = np.expand_dims(xtest_blocks, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOffnzThW4yB",
        "outputId": "59f22f18-d144-4678-dcc9-13ccfcf8322f"
      },
      "outputs": [],
      "source": [
        "y_ = model.predict(xtest_blocks, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PzuvomBW4yB",
        "outputId": "c284e423-8c1e-4fac-cb96-ee9b768bf74e"
      },
      "outputs": [],
      "source": [
        "y_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNoA-8J9W4yB"
      },
      "outputs": [],
      "source": [
        "# These outputs are probabilities\n",
        "# So we apply a threshold.\n",
        "# >= 50% is brain, otherwise it's background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuVT1756W4yB"
      },
      "outputs": [],
      "source": [
        "# This creates a boolean array (values are True or False)\n",
        "brainmask = y_ > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCpfL9C8W4yC"
      },
      "outputs": [],
      "source": [
        "# Remove the last dimension.\n",
        "brainmask = np.squeeze(brainmask, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP35u5DwW4yC",
        "outputId": "d46780a8-5c49-41c7-fc64-34245232124b"
      },
      "outputs": [],
      "source": [
        "brainmask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9bpda4rW4yC"
      },
      "outputs": [],
      "source": [
        "brainmask = nobrainer.volume.from_blocks_numpy(brainmask, output_shape=(256, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "lMUI1VNIW4yC",
        "outputId": "47b9a215-3493-46a9-99c8-fb44aabb8dc7"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "axes[0].matshow(brainmask[120], cmap=\"hot\")\n",
        "axes[1].matshow(brainmask[:, 120], cmap=\"hot\")\n",
        "axes[2].matshow(brainmask[:, :, 120], cmap=\"hot\")\n",
        "axes[3].matshow(xtest[120], cmap=\"gray\")\n",
        "axes[4].matshow(xtest[:, 120], cmap=\"gray\")\n",
        "axes[5].matshow(xtest[:, :, 120], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK_phBSrW4yC"
      },
      "source": [
        "Evaluate the model's performance using Dice score.\n",
        "\n",
        "Is 0.89 for me. Decent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6h7_1IMW4yC",
        "outputId": "6b044e2e-5640-46bc-efb0-1a0a6510f576"
      },
      "outputs": [],
      "source": [
        "ytest_brainmask = ytest > 0\n",
        "\n",
        "nobrainer.metrics.dice(\n",
        "    y_true=ytest_brainmask.astype(\"float32\"), \n",
        "    y_pred=brainmask.astype(\"float32\"), \n",
        "    axis=(0, 1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k373UKyeW4yC"
      },
      "source": [
        "# Lessons so far\n",
        "\n",
        "- Understand your data (visualize your features and labels)\n",
        "- Training is slow\n",
        "- Models do not generalize well\n",
        "\n",
        "# Recommentation\n",
        "\n",
        "If possible, begin with a model that was trained on lots of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl1Wd4o_W4yC"
      },
      "source": [
        "# Pre-trained models\n",
        "\n",
        "**Nobrainer has pre-trained 3D models.** Lots of 2D models pre-trained models exist, but very few 3D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "n7llVv09W4yC",
        "outputId": "2a8ebd03-33ac-41de-919e-7003d0098951"
      },
      "outputs": [],
      "source": [
        "pretrained_model_path = tf.keras.utils.get_file(\n",
        "    fname='brain-extraction-unet-128iso-model.h5',\n",
        "    origin='https://github.com/neuronets/nobrainer-models/releases/download/0.1/brain-extraction-unet-128iso-model.h5')\n",
        "\n",
        "pretrained_model = tf.keras.models.load_model(pretrained_model_path, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-O2TDLaW4yC"
      },
      "outputs": [],
      "source": [
        "pair_of_filepaths = filepaths[1]\n",
        "xtest = nobrainer.io.read_volume(pair_of_filepaths[0])\n",
        "ytest = nobrainer.io.read_volume(pair_of_filepaths[1])\n",
        "\n",
        "xtest = nobrainer.volume.standardize_numpy(xtest)\n",
        "xtest_blocks = nobrainer.volume.to_blocks_numpy(xtest, block_shape=(128, 128, 128))\n",
        "xtest_blocks = np.expand_dims(xtest_blocks, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac2jZnKW4yC"
      },
      "source": [
        "## Predict using the pre-trained brain extraction model\n",
        "\n",
        "Here we are harnessing the knowledge learned by the model from 10,000 different brain scans.\n",
        "\n",
        "The prediction takes about a minute to run. If you cannot run it, here are the outputs.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skbd86NvW4yC"
      },
      "outputs": [],
      "source": [
        "pretrained_outputs = pretrained_model.predict(xtest_blocks, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8NQg6HOW4yD"
      },
      "outputs": [],
      "source": [
        "pretrained_outputs_binary = pretrained_outputs > 0.5\n",
        "pretrained_outputs_binary = pretrained_outputs_binary.squeeze(-1)\n",
        "brainmask = nobrainer.volume.from_blocks_numpy(pretrained_outputs_binary, (256, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixW17GNRW4yD"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "axes[0].matshow(y[120], cmap=\"hot\")\n",
        "axes[1].matshow(y[:, 120], cmap=\"hot\")\n",
        "axes[2].matshow(y[:, :, 120], cmap=\"hot\")\n",
        "axes[3].matshow(x[120], cmap=\"gray\")\n",
        "axes[4].matshow(x[:, 120], cmap=\"gray\")\n",
        "axes[5].matshow(x[:, :, 120], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU0apdt3W4yD"
      },
      "source": [
        "### Assess our performance\n",
        "\n",
        "Dice coefficient is a popular metric of similarity.\n",
        "\n",
        "Dice score is 0.95604503. MUCH better than what we had, and without any training :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-QRXjBfW4yD"
      },
      "outputs": [],
      "source": [
        "ytest_brainmask = ytest > 0\n",
        "\n",
        "nobrainer.metrics.dice(\n",
        "    y_true=ytest_brainmask.astype(\"float32\"), \n",
        "    y_pred=brainmask.astype(\"float32\"), \n",
        "    axis=(0, 1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFc7K0-FW4yD"
      },
      "source": [
        "## Predict using the pre-trained brain segmentation model\n",
        "\n",
        "Here we are harnessing the knowledge learned by the model from over 20,000 different brain scans. This is also a Bayesian model, which can tell us how confident the model is of its own predictions.\n",
        "\n",
        "This model is published in https://doi.org/10.3389/fninf.2019.00067\n",
        "\n",
        "Code is available at https://github.com/neuronets/kwyk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6biRec7kW4yD"
      },
      "source": [
        "**This section takes about five minutes to run on a CPU. Let's skip for now, but here is the output...**\n",
        "\n",
        "This is a subset of FreeSurfer `recon-all` outputs. The major benefit is that you get estimates of confidence and segmentation takes minutes as opposed to hours. However, this has not been validated yet as a tool safe to use in analyses. This is still experimental.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezooLJxNW4yD",
        "outputId": "5fea6727-2b14-4208-ac6e-a601e7d20b94"
      },
      "outputs": [],
      "source": [
        "from nobrainer.models.bayesian import variational_meshnet\n",
        "\n",
        "segmentation_model = variational_meshnet(\n",
        "    n_classes=50, \n",
        "    input_shape=(32, 32, 32, 1),\n",
        "    filters=96, \n",
        "    dropout=\"concrete\", \n",
        "    receptive_field=37, \n",
        "    is_monte_carlo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqueeRC6W4yD",
        "outputId": "5d1c0d4e-656d-48cf-fb5a-d2dd439345fa"
      },
      "outputs": [],
      "source": [
        "weights_path = tf.keras.utils.get_file(\n",
        "    fname=\"nobrainer_spikeslab_32iso_weights.h5\",\n",
        "    origin=\"https://dl.dropbox.com/s/rojjoio9jyyfejy/nobrainer_spikeslab_32iso_weights.h5\")\n",
        "\n",
        "segmentation_model.load_weights(weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZSyhMEZW4yD"
      },
      "outputs": [],
      "source": [
        "pair_of_filepaths = filepaths[1]\n",
        "xtest = nobrainer.io.read_volume(pair_of_filepaths[0])\n",
        "ytest = nobrainer.io.read_volume(pair_of_filepaths[1])\n",
        "\n",
        "xtest = nobrainer.volume.standardize_numpy(xtest)\n",
        "xtest_blocks = nobrainer.volume.to_blocks_numpy(xtest, block_shape=(32, 32, 32))\n",
        "xtest_blocks = np.expand_dims(xtest_blocks, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YztiisHgW4yD",
        "outputId": "69c0ffc8-8ec7-448e-b813-60d02fa3948e"
      },
      "outputs": [],
      "source": [
        "# One a 1080Ti GPU, this takes 16 seconds. On an i7 CPU, this takse 5 minutes.\n",
        "segmentation_outputs = segmentation_model.predict(xtest_blocks, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5LO8yfMW4yD",
        "outputId": "4000bb22-658c-4382-c3be-bc2227d81a43",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Shape is (512, 32, 32, 32, 50)\n",
        "# 512 blocks\n",
        "# 50 classes\n",
        "segmentation_outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5FbLnbiW4yD"
      },
      "outputs": [],
      "source": [
        "# Argmax returns the index of the max value. \n",
        "# So if the max probability is in index 3, then the class is 3.\n",
        "segmentation_outputs_classes = segmentation_outputs.argmax(axis=-1)\n",
        "segmentation_outputs_classes = nobrainer.volume.from_blocks_numpy(\n",
        "    segmentation_outputs_classes, (256, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "FIdgLAI9W4yD",
        "outputId": "41a2b4ae-b7e4-4d37-9a95-c3ce41aec9cd"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "# Remove axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.axis(False)\n",
        "axes[0].matshow(segmentation_outputs_classes[120], cmap=\"hot\")\n",
        "axes[1].matshow(segmentation_outputs_classes[:, 120], cmap=\"hot\")\n",
        "axes[2].matshow(segmentation_outputs_classes[:, :, 120], cmap=\"hot\")\n",
        "axes[3].matshow(x[120], cmap=\"gray\")\n",
        "axes[4].matshow(x[:, 120], cmap=\"gray\")\n",
        "axes[5].matshow(x[:, :, 120], cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEUurtjgW4yD"
      },
      "source": [
        "# The above is great ...\n",
        "\n",
        "We learned how to process our data, instantiate a deep neural network architecture, train our model, and predict. We even learned how to use a pre-trained model. This works well for learning the basics, debugging, and predicting on a few scans.\n",
        "\n",
        "\n",
        "# ... but it does not scale.\n",
        "\n",
        "- If you are training on any respectable amount of data, you will most likely not be able to fit it in memory.\n",
        "- Performing augmentation during training is important, but its not clear how to do that with what we learned so far.\n",
        "- We don't want to load, then process, then train. Would rather load, process, and train in an interleaved fashion, so our CPU and GPU are never idle.\n",
        "    - If you are using paid cloud services, it is especially important to optimize how you spend your time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXNNfmsVW4yD"
      },
      "source": [
        "# Real-world workflow\n",
        "\n",
        "This details the steps for segmentation. Classification and regression are also possible (the labels would be scalars instead of volumetric).\n",
        "\n",
        "1. Prepare data on disk\n",
        "    1. Create a spreadsheet of pairs of features and labels. These are paths to the corresponding MRI files.\n",
        "    2. Separate all of your data into three sets: training, validation, testing\n",
        "        - Train with the training set\n",
        "        - Validate (during training) with the validation set\n",
        "        - Evaluate the model after training on the testing set\n",
        "    3. Convert the data into TFRecord format. \n",
        "        - This format is TensorFlow's preferred data format for data that does not fit into memory. This format allows for parallel reading.\n",
        "2. Create a processing pipeline (or more precisely, a preprocessing graph)\n",
        "    1. Load data from disk\n",
        "    1. Standardize features\n",
        "    2. Prepare labels\n",
        "        - For binary classification, binarize the labels so they are 0 or 1.\n",
        "        - For multi-class classification, map your labels to values in `[0, n_classes-1]`.\n",
        "    3. Optionally add augmentation\n",
        "        - Add Gaussian noise, apply random rigid transformation, etc.\n",
        "        - This can lead to more generalizable models.\n",
        "3. Choose a loss function (and metrics)\n",
        "4. Choose a model\n",
        "    - **Try to find a pre-trained model that suits your needs**\n",
        "    - Instantiate the model from Nobrainer\n",
        "    - Or create the model yourself using the [`tf.keras` API](https://www.tensorflow.org/guide/keras/overview)\n",
        "5. Train (and validate at the end of each epoch)\n",
        "    - This is an iterative process. To improve your \n",
        "6. Evaluate model on testing set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUr0HX22W4yD"
      },
      "source": [
        "## 1. Preparing training data\n",
        "\n",
        "In this tutorial, we will convert medical imaging data to the TFRecords format. Having data in the TFRecords format speeds up training and allows us to use standard, highly-optimized TensorFlow I/O methods. We will then create a `tf.data.Dataset` object using the TFRecords data. This `tf.data.Dataset` object can be used for training, evaluation, or prediction.\n",
        "\n",
        "This tutorial will use publicly available data. To convert your own data, you will need to create a nested list of features and labels volumes. One can store this as a CSV that looks like the following:\n",
        "\n",
        "```\n",
        "features,labels\n",
        "/path/to/1_features.nii.gz,/path/to/1_labels.nii.gz\n",
        "/path/to/2_features.nii.gz,/path/to/2_labels.nii.gz\n",
        "/path/to/3_features.nii.gz,/path/to/3_labels.nii.gz\n",
        "/path/to/4_features.nii.gz,/path/to/4_labels.nii.gz\n",
        "```\n",
        "\n",
        "\n",
        "You can read this CSV in Python with `nobrainer.io.read_csv`.\n",
        "\n",
        "## Google Colaboratory\n",
        "\n",
        "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU is not necessary to prepare the data, but a GPU is helpful for training a model, which we demonstrate at the end of this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu2AX6qOW4yD"
      },
      "source": [
        "## Get sample data\n",
        "\n",
        "Here, we download 10 T1-weighted brain scans and their corresponding FreeSurfer segmentations. These volumes take up about 46 MB and are saved to a temporary directory. The object `csv_path` is the path to a CSV file. Each row of this CSV file contains the paths to a pair of features and labels volumes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebw5ljwfW4yD"
      },
      "outputs": [],
      "source": [
        "csv_of_filepaths = nobrainer.utils.get_data()\n",
        "filepaths = nobrainer.io.read_csv(csv_of_filepaths)\n",
        "\n",
        "# For model training (this is the data the model will learn from).\n",
        "train_paths = filepaths[:7]\n",
        "# For evaluating the model during the training process.\n",
        "# We can change hyperparameters like batch size, learning rate, etc and see the effect on the validation.\n",
        "validation_paths = filepaths[7:9]\n",
        "# For evaluating the final model on unseen data.\n",
        "test_paths = filepaths[9:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9SQD7jaW4yD",
        "outputId": "bd873d8b-7772-4a4a-d998-4ae22287aafa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Verify that all volumes have the same shape and that labels are integer-ish.\n",
        "\n",
        "invalid = nobrainer.io.verify_features_labels(train_paths)\n",
        "assert not invalid\n",
        "\n",
        "invalid = nobrainer.io.verify_features_labels(validation_paths)\n",
        "assert not invalid\n",
        "\n",
        "invalid = nobrainer.io.verify_features_labels(test_paths)\n",
        "assert not invalid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx6cu1TuW4yE"
      },
      "source": [
        "## Convert to volume files to TFRecords\n",
        "\n",
        "To achieve the best performance, training data should be in TFRecords format. This is the preferred file format for TensorFlow, Training can be done on medical imaging volume files but will be slower.\n",
        "\n",
        "Nobrainer has a command-line utility to convert volumes to TFRecords: `nobrainer convert`. This will verify that all of the volumes have the same shape and that the label volumes are an integer type or can be safely coerced to an integer type. \n",
        "\n",
        "Following successful verification, the volumes will be converted to TFRecords files. The dataset should be sharded into multiple TFRecords files, so that data can be shuffled more properly. This is especially helpful for large datasets. Users can choose how many pairs of volumes (i.e., features and labels) will be saved to one TFRecords file. In this example, we will save 3 pairs of volumes per TFRecords file because our dataset is small. With a larger dataset, users should choose a larger shard value. For example, with 10,000 volumes, one might choose 100 volumes per TFRecords file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "UzSre9KkW4yE",
        "outputId": "5abd2ac3-3b46-4340-f723-e4069b6dc816"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Convert training and evaluation data to TFRecords.\n",
        "\n",
        "nobrainer.tfrecord.write(\n",
        "    features_labels=train_paths,\n",
        "    filename_template='data/data-train_shard-{shard:03d}.tfrec',\n",
        "    examples_per_shard=3)\n",
        "\n",
        "nobrainer.tfrecord.write(\n",
        "    features_labels=validation_paths,\n",
        "    filename_template='data/data-validation_shard-{shard:03d}.tfrec',\n",
        "    examples_per_shard=1)\n",
        "\n",
        "nobrainer.tfrecord.write(\n",
        "    features_labels=test_paths,\n",
        "    filename_template='data/data-test_shard-{shard:03d}.tfrec',\n",
        "    examples_per_shard=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bolXAafjW4yE"
      },
      "outputs": [],
      "source": [
        "os.listdir(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZZqsWBeW4yE"
      },
      "source": [
        "# 2. Create input data pipeline\n",
        "\n",
        "We will now create an data pipeline to feed our models with training data. The steps below will create a `tensorflow.data.Dataset` object that is built according to [TensorFlow's guidelines](https://www.tensorflow.org/guide/performance/datasets). The basic pipeline is summarized below.\n",
        "\n",
        "- Read data\n",
        "- Separate volumes into non-overlapping sub-volumes\n",
        "    - This is done to get around memory limitations with larger models.\n",
        "    - For example, a volume with shape (256, 256, 256) can be separated into eight non-overlapping blocks of shape (128, 128, 128).\n",
        "- Apply random rigid augmentations if requested.\n",
        "- Standard score volumes of features.\n",
        "- Binarize labels if binary segmentation.\n",
        "- Replace values according to some mapping if multi-class segmentation.\n",
        "- Batch the results so every iteration yields `batch_size` elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSNuD1OrW4yE"
      },
      "outputs": [],
      "source": [
        "# The number of classes the model predicts. A value of 1 means the model performs\n",
        "# binary classification (i.e., target vs background).\n",
        "n_classes = 1\n",
        "\n",
        "# Batch size is the number of features and labels we train on with each step.\n",
        "batch_size = 2\n",
        "\n",
        "# The shape of the original volumes.\n",
        "volume_shape = (256, 256, 256)\n",
        "\n",
        "# The shape of the non-overlapping sub-volumes. Most models cannot be trained on\n",
        "# full volumes because of hardware and memory constraints, so we train and evaluate\n",
        "# on sub-volumes.\n",
        "block_shape = (128, 128, 128)\n",
        "\n",
        "# Whether or not to apply random rigid transformations to the data on the fly.\n",
        "# This can improve model generalizability but increases processing time.\n",
        "augment = False\n",
        "\n",
        "# Number of epochs for which to repeat the dataset.\n",
        "n_epochs = 5\n",
        "\n",
        "# The tfrecords filepaths will be shuffled before reading, but we can also shuffle\n",
        "# the data. This will shuffle 10 volumes at a time. Larger buffer sizes will require\n",
        "# more memory, so choose a value based on how much memory you have available.\n",
        "shuffle_buffer_size = 10\n",
        "\n",
        "# Number of parallel processes to use. Let tensorflow figure it out...\n",
        "num_parallel_calls = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xpO7KkhW4yE"
      },
      "outputs": [],
      "source": [
        "dset_train = nobrainer.dataset.get_dataset(\n",
        "    file_pattern=\"data/data-train_shard-*.tfrec\",\n",
        "    n_classes=n_classes,\n",
        "    batch_size=batch_size,\n",
        "    volume_shape=volume_shape,\n",
        "    block_shape=block_shape,\n",
        "    augment=augment,\n",
        "    n_epochs=1,  # Set this to 1 because `model.fit` will repeat on its own.\n",
        "    shuffle_buffer_size=shuffle_buffer_size,\n",
        "    num_parallel_calls=num_parallel_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Qp2Wb_n-W4yE",
        "outputId": "5c7e7ee2-edd7-43f3-b81f-2f702ead6a7f"
      },
      "outputs": [],
      "source": [
        "dset_validation = nobrainer.dataset.get_dataset(\n",
        "    file_pattern=\"data/data-validation_shard-*.tfrec\",\n",
        "    n_classes=n_classes,\n",
        "    batch_size=batch_size,\n",
        "    volume_shape=volume_shape,\n",
        "    block_shape=block_shape,\n",
        "    augment=False,\n",
        "    n_epochs=1,\n",
        "    shuffle_buffer_size=None,\n",
        "    num_parallel_calls=num_parallel_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "vuqdd-rQW4yE",
        "outputId": "90ad1231-93cf-40c3-a8a2-973c2af6d9a5"
      },
      "outputs": [],
      "source": [
        "dset_test = nobrainer.dataset.get_dataset(\n",
        "    file_pattern=\"data/data-test_shard-*.tfrec\",\n",
        "    n_classes=n_classes,\n",
        "    batch_size=batch_size,\n",
        "    volume_shape=volume_shape,\n",
        "    block_shape=block_shape,\n",
        "    augment=False,\n",
        "    n_epochs=1,\n",
        "    shuffle_buffer_size=None,\n",
        "    num_parallel_calls=num_parallel_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48lYhhU7W4yF"
      },
      "source": [
        "# 3. Choose a loss function\n",
        "\n",
        "Many options... Dice, Jaccard, Tversky, cross-entropy, and many others.\n",
        "\n",
        "See `nobrainer.losses` and please submit an [issue](https://github.com/neuronets/nobrainer/issues/new/choose) or [pull request](https://github.com/neuronets/nobrainer/pulls) if you run into issues or want to add functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWmtOFOcW4yF"
      },
      "source": [
        "# 4. Choose a model\n",
        "\n",
        "Even more options than loss functions. Nobrainer has several implementations, including U-Net, MeshNet, Bayesian U-Net, and Highresnet.\n",
        "\n",
        "**If possible, use a pre-trained model. This is especially important if you do not have much data.**\n",
        "\n",
        "If you need to train a model from scratch, a good heuristic for choosing an architecture is to look at segmentation challenges (like MICCAI BraTS) and see what performed well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwesJFSRW4yF"
      },
      "source": [
        "See https://github.com/neuronets/nobrainer-models#brain-extraction for information about the pre-trained model.\n",
        "\n",
        ">This model achieved a median Dice score of 0.97, mean of 0.96, minimum of 0.91, and maximum of 0.98 on a validation dataset of 99 T1-weighted brain scans and their corresponding binarized FreeSurfer segmentations (public and private sources). This model should be agnostic to orientation and can predict the brainmask for a volume of size 256x256x256 in approximately three seconds. The model was trained on 10,000 brain scans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPKmvw9jW4yF"
      },
      "outputs": [],
      "source": [
        "model_path = tf.keras.utils.get_file(\n",
        "    fname='brain-extraction-unet-128iso-model.h5',\n",
        "    origin='https://github.com/neuronets/nobrainer-models/releases/download/0.1/brain-extraction-unet-128iso-model.h5')\n",
        "\n",
        "model = tf.keras.models.load_model(model_path, compile=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74orWQzcW4yF"
      },
      "source": [
        "## Considerations for transfer learning\n",
        "\n",
        "Training a neural network changes the model's weights. A pre-trained network has learned weights for a task, and we do not want to forget these weights during training. In other words, we do not want to ruin the pre-trained weights when using our new data. To avoid dramatic changes in the learnable parameters, we can apply regularization and use a relatively small learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBqxJqAcW4yF"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    layer.kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-05)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=nobrainer.losses.jaccard,\n",
        "    metrics=[nobrainer.metrics.dice])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qap96mU6W4yF"
      },
      "source": [
        "# 5. Train\n",
        "\n",
        "Notes: during the first epoch, the number of steps will be unknown (`tf.data.Dataset` does not know how many steps it contains), but it figures it out after the first epoch. \n",
        "\n",
        "In this case, each epoch is 28 steps. \n",
        "\n",
        "$$\n",
        "steps = \\frac{nBlocks}{volume} * \\frac{nVolumes}{batchSize} = \\frac{8}{1} * \\frac{7}{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM6V4Q_BW4yF"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    dset_train,\n",
        "    validation_data=dset_validation,\n",
        "    epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYACg9idW4yF"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP4hbXvoW4yF"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
        "plt.plot(history.history[\"dice\"], label=\"Dice\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss or Dice\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94AP9ZPAW4yF"
      },
      "source": [
        "# 6. Evaluate\n",
        "\n",
        "Calculate Dice score on an unseen scan.\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "      4/Unknown - 3s 627ms/step - loss: 0.0888 - dice: 0.9532\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX9sWG9KW4yF",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "output = model.evaluate(dset_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
